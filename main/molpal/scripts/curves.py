from argparse import ArgumentParser
from itertools import repeat
from typing import Iterable

from matplotlib import pyplot as plt
from matplotlib import ticker
import numpy as np
import seaborn as sns

from experiment import Experiment
from utils import (
    extract_smis, build_true_dict, chunk, style_axis, abbreviate_k_or_M
)

sns.set_theme(style='white', context='paper')

def plot_reward_curves(yss: Iterable[Iterable[np.ndarray]],
                       labels: Iterable[str], bs: int, title: str):
    fig, ax = plt.subplots(1, 1, sharey=True, figsize=(4, 4))
    fmt = '-'

    y_min, y_max = 0, 0
    for ys, label in zip(yss, labels):
        Y = np.stack(ys)
        y_mean = Y.mean(axis=0)
        y_sd = Y.std(axis=0)

        y_max = max(y_max, max(y_mean))
        y_min = max(y_min, max(y_mean))

        x = np.arange(len(y_mean)) + 1
        ax.plot(x, y_mean, fmt, label=label, alpha=0.9)
        if len(Y) >= 3:
            ax.fill_between(x, y_mean-y_sd, y_mean+y_sd, alpha=0.3)

    n_iters = len(x) // bs
    ax.vlines([bs * (i+1) for i in range(n_iters)], y_min, y_max,
              color='r', ls='dashed', lw=0.5)
    formatter = ticker.FuncFormatter(abbreviate_k_or_M)
    ax.xaxis.set_major_formatter(formatter)
    style_axis(ax)
    ax.set_ylabel(title)
    ax.legend(loc='lower right')

    fig.tight_layout()

    return fig

#-----------------------------------------------------------------------------#

if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument('-e', '--experiments', '--expts', nargs='+',
                        help='the top-level directory generated by the MolPAL run. I.e., the directory with the "data" and "chkpts" directories')
    parser.add_argument('-l', '--library',
                        help='the library file used for the corresponding MolPAL run.')
    parser.add_argument('--true-csv',
                        help='a pickle file containing a dictionary of the true scoring data')
    parser.add_argument('--smiles-col', type=int, default=0)
    parser.add_argument('--score-col', type=int, default=1)
    parser.add_argument('--no-title-line', action='store_true', default=False)
    parser.add_argument('--maximize', action='store_true', default=False,
                        help='whether the objective for which you are calculating performance should be maximized.')
    parser.add_argument('-m', '--metrics', nargs='+', default=repeat('greedy'),
                        help='the respective acquisition metric used for each experiment')
    parser.add_argument('-k', type=int,
                        help='')
    parser.add_argument('-N', type=int,
                        help='the number of top scores from which to calculate the reward')
    parser.add_argument('-r', '--reward',
                        choices=('scores', 'smis', 'top-k-ave', 'total-ave'),
                        help='the type of reward to calculate')
    parser.add_argument('--split', type=float, default=0.004,
                        help='the split size to plot when using model-metrics mode')
    parser.add_argument('--reps', type=int, nargs='+',
                        help='the number of reps for each configuration. I.e., you passed in the arguments: --expts e1_a e1_b e1_c e2_a e2_b where there are three reps of the first configuration and two reps of the seecond. In this case, you should pass in: --reps 3 2. By default, the program assumed each experiment is a unique configuration.')
    parser.add_argument('--labels', nargs='+',
                        help='the label of each trace on the plot. Will use the metric labels if not specified. NOTE: the labels correspond the number of different configurations. I.e., if you pass in the args: --expts e1_a e1_b e1_c --reps 3, you only need to specify one label: --labels l1')
    parser.add_argument('--name',
                        help='the filepath to which the plot should be saved')

    args = parser.parse_args()
    args.title_line = not args.no_title_line

    smis = extract_smis(args.library, args.smiles_col, args.title_line)
    d_smi_idx = {smi: i for i, smi in enumerate(smis)}

    d_smi_score = build_true_dict(
        args.true_csv, args.smiles_col, args.score_col,
        args.title_line, args.maximize
    )

    true_smis_scores = sorted(d_smi_score.items(), key=lambda kv: kv[1])[::-1]
    true_top_k = true_smis_scores[:args.N]

    reward_curves = []
    # init_sizes = []
    for experiment in args.experiments:
        experiment = Experiment(experiment, d_smi_idx)
        # init_sizes.append(experiment.init_size)
        reward_curves.append(experiment.reward_curve(true_top_k, args.reward))

    reward_curvess = chunk(reward_curves, args.reps or [])
    # init_sizes = [x[0] for x in chunk(init_sizes, args.reps or [])]

    bs = int(args.split * len(smis))
    title = {
        'smis': f'Percentage of Top-{args.N} SMILES Found',
        'scores': f'Percentage of Top-{args.N} scores Found',
        'top-k-ave': f'Top-{args.N} average',
        'total-ave': f'Total average'
    }[args.reward]

    fig = plot_reward_curves(
        reward_curvess, args.labels or args.metrics, bs, title
    ).savefig(args.name)

    exit()